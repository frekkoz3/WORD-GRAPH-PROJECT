{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD GRAPH PROJECT - step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the work done here \n",
    "\n",
    "https://www.kaggle.com/datasets/moxxis/harry-potter-lstm?resource=download&select=Harry_Potter_all_books_preprocessed.txt\n",
    "\n",
    "so thank to this boy to provide all the data we searched and to prepocess even them (this should help a lot). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by reading everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_file = open(\"HP_books.txt\", \"r\")\n",
    "HP_text = HP_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split everything and count how many words there are. We also replace some character with a space (since we had some problem with some sentences) and we lower everything. We set | as the ending character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1259308"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_char = ['.', ',', '?', '!', 'â€˜', 'â€¢', '~', '(', ')']\n",
    "for s_char in special_char:\n",
    "    HP_text = HP_text.replace(s_char, f\" {s_char} | \")\n",
    "HP_text = HP_text.lower()\n",
    "HP_words = HP_text.split()\n",
    "len(HP_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we count the numbers of single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23232"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HP_set_words = set(HP_words)\n",
    "len(HP_set_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is something really cool to notice. We now there are approximetely 170.000 words in the english vocabulary and this means that HP really minimize the space of the words (relatively talking).\n",
    "Now we want to count the occurrencies of the single words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_occourrencies_dict = {}\n",
    "for word in HP_words:\n",
    "    if word == '':\n",
    "        continue\n",
    "    if word in HP_occourrencies_dict.keys():\n",
    "        HP_occourrencies_dict[word] += 1\n",
    "    else:\n",
    "        HP_occourrencies_dict[word] = 1\n",
    "HP_sorted_occourrencies_dict = dict(sorted(HP_occourrencies_dict.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| occourred 86068\n",
      ". occourred 67794\n",
      "the occourred 51921\n",
      "and occourred 27607\n",
      "to occourred 26852\n",
      "of occourred 21843\n",
      "a occourred 21074\n",
      "he occourred 20430\n",
      "harry occourred 16750\n",
      "was occourred 15644\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "lim = 10\n",
    "for unique_word in HP_sorted_occourrencies_dict:\n",
    "    if k < lim:\n",
    "        print(f\"{unique_word} occourred {HP_sorted_occourrencies_dict[unique_word]}\")\n",
    "        k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a class for the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class WordGraph:\n",
    "    def __init__(self, text): # we assume the text is already prepocessed\n",
    "        self.text = text\n",
    "        self.words = self.text.split()\n",
    "        self.unique_word = set(self.words)\n",
    "        self.words_dict = {u_word : {} for u_word in self.unique_word}\n",
    "        self.compute_graph()\n",
    "    \n",
    "    def compute_graph(self):\n",
    "        for predecessor, successor in zip(self.words[:-1], self.words[1:]):\n",
    "            if predecessor == '' or successor == '':\n",
    "                continue\n",
    "            if successor not in self.words_dict[predecessor].keys():\n",
    "                self.words_dict[predecessor][successor] = 1\n",
    "            else:\n",
    "                self.words_dict[predecessor][successor] += 1\n",
    "    \n",
    "    def print_graph(self):\n",
    "        for words in self.words_dict:\n",
    "            print(f\"{words} \")\n",
    "            for word in self.words_dict[words]:\n",
    "                print(f\" |->{word} : {self.words_dict[words][word] }\")\n",
    "            print(\" \")\n",
    "    \n",
    "    def print_word(self, key_word):\n",
    "        key_word = key_word.lower()\n",
    "        print(f\"{key_word} \")\n",
    "        try:\n",
    "            w_dict = self.words_dict[key_word]\n",
    "        except:\n",
    "            print(\" |->Word not found\")\n",
    "            return\n",
    "        for word in w_dict:\n",
    "            print(f\" |->{word} : {self.words_dict[key_word][word] }\")\n",
    "    \n",
    "    def generate_from_word(self, key_word, max_deep = 15):\n",
    "        key_word = key_word.lower()\n",
    "        if max_deep == 0 or key_word == '|':\n",
    "            print()\n",
    "            return\n",
    "        print(key_word, end=' ')\n",
    "        try:\n",
    "            w_dict = self.words_dict[key_word]\n",
    "        except:\n",
    "            print(\"Word not found\")\n",
    "            return \n",
    "        cumulative_space = {}\n",
    "        c = 0 \n",
    "        for word in w_dict:\n",
    "            c = c + self.words_dict[key_word][word]\n",
    "            cumulative_space[c] = word\n",
    "        index = random.randint(0, c)\n",
    "        for ind in cumulative_space.keys(): # we could optimize this with a binary search. now it's not needed.\n",
    "            if index < ind:\n",
    "                self.generate_from_word(cumulative_space[ind], max_deep - 1)\n",
    "                return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_wg = WordGraph(HP_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement also a function called \"generate from word\" that simply, given a word, create a cumulative probabilistic function and generate from it the next word. the cumulative function just use the sum of the occurrencies in order. sampled by it we take the correspective word: if it finds an end character (or goes too deep in the recursion) it terminates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi she said hermione muttered alohomora . \n"
     ]
    }
   ],
   "source": [
    "HP_wg.generate_from_word('hi', max_deep=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
